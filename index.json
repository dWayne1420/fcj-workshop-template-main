[{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction Elastic Beanstalk (EB) is an application management service (Platform as a Service – PaaS) provided by AWS, enabling developers to deploy and operate web applications without directly managing server infrastructure.\nIn this workshop, a Spring Boot REST API application will be deployed on Elastic Beanstalk using the Single Instance configuration and the Default VPC, ensuring simplicity, ease of deployment, and suitability for learning, experimentation, and technical demonstration.\nThe deployment process focuses on the following core steps:\nPackaging the Spring Boot application into an executable .jar file. Creating an Application and Environment on Elastic Beanstalk. Uploading and deploying the application version. Testing the service using Postman. Monitoring system logs to evaluate application behavior and troubleshoot issues. Through this workshop, participants will understand the standardized process of deploying a Java backend service on AWS, as well as how Elastic Beanstalk manages the application lifecycle in a Single Instance environment.\nThis also serves as a foundation for expanding into more advanced scenarios such as Load Balanced Environments, integrating RDS, or implementing CI/CD in future workshops.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Worklog Introduction Overview This Worklog records my entire 12-week journey of learning and practicing AWS.\nThe goal was to combine theoretical knowledge from the course materials\n(reference: https://cloudjourney.awsstudygroup.com/) with hands-on practice\ndirectly on the AWS platform (using Free Tier whenever possible) to build the\nskills required to deploy, operate, and optimize cloud systems at a\nproduction-ready level.\nHow I Completed It Learning approach: each week I focused on a specific group of topics\n(e.g., IAM \u0026amp; VPC, EC2 \u0026amp; LAMP, S3 \u0026amp; CloudFront, \u0026hellip;), studied the reference\nmaterials, and practiced directly on AWS using both the Console and\nAWS CLI. Structured practice: every week contained a clear task table\n(Day / Task / Start / Completion / Ref), and each task was completed through\nsteps involving setup, configuration, testing, and logging results. Documentation \u0026amp; reporting: all operations, CLI commands, configurations,\nand test results were recorded in the weekly Worklog to ensure reproducibility\nand easy review. Testing \u0026amp; optimization: after deploying resources, I performed tests\n(connectivity, basic load tests, health checks), monitored metrics via\nCloudWatch, and tuned configurations (SG rules, sizing, lifecycle rules, etc.)\nto improve performance and cost efficiency. Timeframe Total duration: 12 weeks (starting from 09/09/2025 according to Week 1). Weekly workload: each week contained structured tasks (usually 5 tasks per\nweek) — combining theory and practice, typically completing 1–2 tasks per day\ndepending on complexity. Weekly Summary Week 1 — AWS Foundations: Created Free Tier account, built IAM baseline,\ninstalled Hugo, explored Console, configured AWS Budgets and AWS CLI. Week 2 — IAM \u0026amp; VPC: Managed IAM (users/groups/roles/policies), built VPC\n(public/private subnets), configured SGs \u0026amp; NACLs, deployed EC2, studied\nsite-to-site VPN. Week 3 — EC2 Advanced \u0026amp; LAMP/Node.js: Explored EC2 internals, installed\nLAMP stack, deployed Node.js apps, implemented IAM cost governance. Week 4 — S3 \u0026amp; CloudFront: Managed S3 buckets, hosted static website,\nintegrated CloudFront, enabled versioning, lifecycle rules, CRR. Week 5 — RDS / Lightsail / DynamoDB / SDK: Deployed RDS, Lightsail\ndatabase, Lightsail containers, learned DynamoDB, wrote scripts using AWS SDK. Week 6 — Architecture \u0026amp; Database Design: Researched AWS services, designed\nfull architecture, created ERD/schema, completed technical documentation. Week 7 — CloudWatch / SNS / SQS: Configured logs, metrics, dashboards,\nalarms; integrated SNS; built SQS workflows. Week 8 — Elastic Beanstalk \u0026amp; Auto Scaling: Deployed multi-tier app on EB,\nconfigured ALB, Auto Scaling, deployment lifecycle. Week 9 — Serverless (Lambda \u0026amp; API Gateway): Built Lambda functions,\ncreated REST APIs, integrated with S3/DynamoDB/SNS/EventBridge, configured IAM\nfor serverless. Week 10 — CI/CD (CodeCommit/CodeBuild/CodeDeploy/CodePipeline): Built full\nCI/CD pipeline, wrote buildspec.yml and appspec.yml, tested end-to-end flow. Week 11 — Containers (Docker, ECR, ECS, Fargate): Created Docker images,\npushed to ECR, deployed via ECS EC2/Fargate, configured networking \u0026amp;\nauto-scaling. Week 12 — Final Deployment \u0026amp; Optimization: Deployed final project,\nintegrated monitoring and CI/CD, optimized performance/cost/security,\nperformed load tests, completed full documentation. Outcomes Achieved Ability to deploy and operate an end-to-end cloud application system on AWS\n(compute, storage, database, networking, monitoring). Proficiency in managing AWS using both Console and CLI, and combining them\nfor automation. Built complete CI/CD pipelines; deployed serverless and containerized\nworkloads. Produced a full set of architecture documents, database designs, and\ndeployment guidelines suitable for reproducing and scaling the project. Main Reference Theoretical resources \u0026amp; hands-on guides: https://cloudjourney.awsstudygroup.com/ "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/4-eventparticipated/4.1-event-1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “GenAI-powered App-DB Modernization Workshop” Event Objectives Introduce how Generative AI is reshaping the entire software development lifecycle. Showcase Amazon Q Developer’s features for coding, testing, documentation, and architectural support. Demonstrate how Kiro enhances DevOps and operations through log analysis, troubleshooting, and automation. Explain the AI-Driven Development model and how teams can incorporate AI into daily workflows. Provide practical demonstrations to help participants understand how to apply these tools effectively. Speakers Toan Huynh – PMP, Senior Solutions Architect (AWS) My Nguyen – Senior Solutions Architect | Applied AI Specialist | Thought Leader Key Highlights Understanding common challenges in legacy systems such as limited scalability, high maintenance costs, tight coupling, and slow release cycles. Scalability limitations – systems struggle to meet increasing user demand. High maintenance costs – older systems require more resources and effort to keep running. Tight coupling – components depend on each other, making changes risky. Identifying the drawbacks of legacy application architecture Long product release cycles → lost revenue and missed opportunities Inefficient operations → reduced productivity and higher costs Non-compliance with security standards → increased risk and reputational damage Transitioning to Modern Architecture – Microservices Modernization involves shifting to a modular architecture in which each feature is an independent service communicating through events. This approach is built on three core principles:\nQueue Management: Manages asynchronous tasks reliably and prevents workflow bottlenecks. Caching Strategy: Improves performance, minimizes latency, and reduces load on databases. Message Handling: Enables flexible, event-driven communication through pub/sub, point-to-point, and streaming patterns. Domain-Driven Design (DDD) – Workshop Notes Overview:\nDDD aligns software design with real business needs, making large systems easier to manage. The workshop leveraged DDD alongside AI tools like Amazon Q Developer to support modernization.\nFour-Step Method (from the workshop):\nIdentify Domain Events – Capture key business events that drive system behavior. Arrange Timeline – Lay out the events in chronological order to visualize workflows. Identify Actors – Identify the users or systems that interact with each event. Define Bounded Contexts – Divide the system into clear, independent domains to support scalability. Workshop Case Study – Bookstore Example:\nDemonstrated practical DDD application. Domain events included: “Order Placed”, “Payment Completed”, “Book Shipped”. Bounded contexts: Order Management, Payment Processing, Inventory. Context Mapping – 7 Patterns Introduced:\nShared Kernel, Customer-Supplier, Conformist, Anticorruption Layer, Open Host Service, Published Language, Separate Ways.\nKey Takeaways:\nDDD helps reduce tight coupling and increases clarity in system design. Amazon Q Developer supports modeling, generating code, and producing documentation for bounded contexts. Context mapping ensures smooth communication across microservices during modernization. Event-Driven Architecture Integration patterns: Publish/Subscribe, Point-to-Point, Streaming Benefits: reduced coupling, higher scalability, improved resilience Sync vs Async: trade-offs based on performance and reliability requirements Compute Evolution Shared Responsibility Progression: EC2 → ECS → Fargate → Lambda Serverless Benefits: no server management, automatic scaling, pay-per-use model Choosing Compute: select functions or containers based on workload characteristics Amazon Q Developer Automates the entire SDLC: planning → coding → testing → deployment → maintenance Supports code transformation: Java upgrades, .NET modernization Works with AWS Transform agents for VMware, Mainframe, and .NET migrations Key Takeaways Design Mindset Adopt a business-first approach to guide architecture decisions. Establish a ubiquitous language shared by business and technical teams. Use bounded contexts to handle complexity in large systems. Technical Architecture Apply event storming to model business processes effectively. Use event-driven communication to reduce coupling. Select appropriate integration patterns: sync, async, pub/sub, streaming. Choose compute options—VMs, containers, or serverless—based on workload needs. Modernization Strategy Follow a phased modernization roadmap to avoid unnecessary risk. Use the 7Rs Framework to choose the right modernization path for each application. Measure ROI to ensure business value and cost reduction. Applying to Work Conduct event storming sessions with business stakeholders. Refactor systems into microservices using bounded contexts. Implement event-driven communication to replace certain synchronous workflows. Adopt serverless using AWS Lambda for appropriate workloads. Integrate Amazon Q Developer into development workflows to improve productivity. Event Experience Learning from Speakers Gained actionable insights into modern application design and AWS best practices. Understood how DDD and Event-Driven Architecture work in real-world scenarios. Hands-On Exposure Participated in event storming sessions to map domain events. Practiced splitting microservices and defining bounded contexts. Explored communication patterns including sync, async, pub/sub, and streaming. Leveraging Tools Experimented with Amazon Q Developer for SDLC automation. Tried code transformation and serverless deployment using AWS Lambda. Networking and Discussions Engaged with experts and peers to exchange ideas and perspectives. Strengthened communication between business and technical teams. Saw how the business-first mindset enhances architecture decisions. Lessons Learned Combining DDD with event-driven patterns reduces coupling and improves scalability and resilience. Modernization requires a structured, phased plan with clear ROI tracking. AI tools like Amazon Q Developer significantly boost productivity when integrated into workflows. Overall, the workshop delivered valuable technical knowledge and reshaped perspectives on application design, modernization strategy, and cross-team collaboration.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Build foundational understanding of AWS Cloud concepts. Create and configure an AWS Free Tier account. Establish IAM security baseline for account access. Install and configure Hugo for documentation. Learn and navigate the AWS Management Console. Set up AWS Budgets for cost monitoring. Begin using AWS CLI for basic operations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Creating my first AWS account 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 2 Create Admin Group and Admin User 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 3 Setting Hugo 09/10/2025 09/10/2025 4 Explore AWS Management Console 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 5 Create Budget by Template 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood core AWS Cloud concepts, including:\nGlobal Infrastructure (Regions, AZs, Edge Locations) AWS Shared Responsibility Model Service categories:\nCompute, Storage, Networking, Databases, Security, Developer Tools, Monitoring Practical understanding sourced from\n➝ https://cloudjourney.awsstudygroup.com/ Successfully created and configured an AWS Free Tier account, including:\nEmail + billing verification MFA setup for root account Initial security best practices for new accounts Built a secure IAM foundation, including:\nCreated Admin Group (AdministratorAccess) Created Admin User and assigned permissions Logged in using IAM user instead of root Secured the account following AWS-recommended guidelines Mastered basic AWS Management Console operations:\nLearned how services are grouped Navigated dashboards, search bar, and resource menus Found and opened services such as EC2, S3, IAM, VPC Installed and configured Hugo for documentation:\nInstalled Hugo CLI Created and initialized Hugo site folder structure Added initial content and tested local build Set up AWS Budgets for cost monitoring:\nCreated Free Tier cost alerts Configured email notifications Understood core concepts of AWS billing Installed and configured AWS CLI, including:\nAdded Access Key + Secret Key Configured default region + output format Performed essential AWS CLI operations, such as:\naws configure list aws ec2 describe-regions aws ec2 describe-instances aws ec2 describe-key-pairs Checked identity via sts get-caller-identity Gained the ability to work with AWS via both Console and CLI, managing services in parallel.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"Prepare and create the build file (.jar) Before deploying to AWS, the Spring Boot application needs to be packaged into an executable .jar file.\nIn the project’s root directory, run the command:\nmvn clean package -DskipTests After the process completes, the built file will appear in the directory:\n/target/\u0026lt;application-name\u0026gt;.jar "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/2-proposal/","title":"Proposal","tags":[],"description":"","content":"In this section, you need to summarize the contents of the workshop that you plan to conduct.\nSportShop E-Commerce Platform A Cost-Optimized AWS Three-Tier Architecture for Online Sports Retail 1. Executive Summary The SportShop E-Commerce Platform is designed to modernize online retail operations using a simplified and cost-optimized AWS architecture, suitable for student projects and small-scale applications. The system uses ReactJS for the frontend, Spring Boot for the backend, and Amazon RDS MySQL for persistent data storage.\nTo reduce cost and complexity, the architecture removes Amazon Cognito, the Application Load Balancer (ALB), NAT Gateway, and banking payment integration. User authentication is handled directly by the backend, which runs on Elastic Beanstalk single-instance mode, while static content is delivered through S3 + CloudFront.\nDespite being simplified, the architecture still follows AWS best practices for security and CI/CD using ACM, Parameter Store, CloudWatch, CodePipeline, and CodeBuild.\n2. Problem Statement Current Problems Slow, manual, and inconsistent deployment workflows Risk of leaked or improperly stored credentials Limited system monitoring and alerting High chance of downtime during updates Proposed Solution The platform leverages AWS managed services to automate deployments, enhance security, and implement a clean three-layer architecture:\nEdge: Route 53, CloudFront (ACM) Application: Elastic Beanstalk (single-instance) Data: RDS MySQL, S3, CloudWatch Secrets such as database credentials and JWT signing keys are securely stored in Parameter Store.\nCI/CD is managed through GitLab → CodePipeline → CodeBuild.\nBenefits Lower AWS cost and reduced architectural complexity Faster deployments through automation Global HTTPS delivery through CloudFront Clear separation between frontend, backend, and database layers 3. Solution Architecture AWS Services Used Amazon Route 53 — DNS Amazon CloudFront — CDN and HTTPS distribution AWS Certificate Manager — SSL/TLS certificates Elastic Beanstalk (Single Instance) — Backend hosting Amazon RDS MySQL — Main database Amazon S3 — Static hosting \u0026amp; media storage Parameter Store — Secure secret management CloudWatch — Logs and metrics CodePipeline + CodeBuild — CI/CD automation Layered Architecture Edge Layer\nRoute 53 → CloudFront CloudFront serves the ReactJS frontend globally over HTTPS Application Layer\nSpring Boot backend deployed on Elastic Beanstalk single-instance Authentication handled with JWT Secure configuration loaded from Parameter Store Data Layer\nRDS MySQL stores structured application data S3 stores static content and media files CloudWatch provides centralized monitoring 4. Technical Implementation Configure VPC, subnets, and security groups Host frontend on S3 + CloudFront Deploy backend with Elastic Beanstalk Provision RDS MySQL Store secrets in Parameter Store CI/CD pipeline: GitLab → CodePipeline → CodeBuild Enable monitoring via CloudWatch Tech Stack:\nFrontend: ReactJS (S3 + CloudFront) Backend: Spring Boot (Java 17) Database: Amazon RDS MySQL CI/CD: AWS CodePipeline, CodeBuild Security: IAM, ACM, Parameter Store, CloudWatch 5. Timeline \u0026amp; Milestones Month 1 Set up AWS environment (VPC, RDS, S3, CloudFront, Route 53) Deploy backend \u0026amp; frontend Configure CI/CD pipeline Month 2 Implement JWT authentication Develop modules: product, order, user management Set up monitoring \u0026amp; alerting Month 3 Testing and optimization Final deployment Documentation and training 6. Budget Estimation You can see the cost on the AWS Pricing Calculator.\nAWS Service Monthly Cost Amazon RDS MySQL $21.84 Elastic Beanstalk (EC2 t3.micro) $11.68 Amazon S3 $0.26 Data Transfer $0.60 CloudFront $0.88 Route 53 $0.90 ACM Public Certificate $0 CloudWatch Metrics $3.02 CodePipeline $0 CodeBuild $0.50 Parameter Store $0 CloudWatch Logs $1.41 Estimated total: ~$35–40 per month\nYearly: ~$420–480\n7. Risk Assessment Risks Single-instance backend → No fault tolerance No NAT Gateway → Backend cannot call external APIs Risk of JWT token misuse without proper security Mitigation Enable Elastic Beanstalk auto-recovery and health checks Apply secure authentication \u0026amp; token handling practices Utilize RDS automated backups and snapshots 8. Future Enhancements Amazon Cognito — MFA, OAuth, Social Login Application Load Balancer — Multi-instance scaling \u0026amp; high availability NAT Gateway — Allow backend to call external APIs Payment Integrations: VNPay, Stripe, PayPal RDS Multi-AZ — High availability and automatic failover S3 Lifecycle + Glacier — Cost-optimized long-term storage AWS X-Ray + OpenSearch — Tracing and advanced logging Microservices Architecture with ECS/EKS — Scale backend into multiple services "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/4-eventparticipated/4.2-event-2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “Defense from Public Threat: AWS WAF \u0026amp; Application Protection” Event Objectives Highlight common security risks for internet-facing applications and outline effective mitigation strategies. Explain AWS WAF operations, deployment best practices, and advanced rule configurations. Describe AWS WAF Bot Control capabilities for defending against automated threats. Provide an overview of AWS Shield’s DDoS protection mechanisms and enhancements, including the new flat-rate pricing model for bundled services. Speakers Nguyen Gia Hung Julian Ju Key Highlights Security Risks in a Typical 3-Tier Web Architecture Direct internet exposure introduces vulnerabilities that may result in prolonged outages, resource exhaustion, and compliance failures. Business impacts include elevated infrastructure spend, data breaches, leaked credentials, spam surges, downtime, operational instability, staff fatigue, and loss of customer trust. Major Categories of Internet Threats Denial of Service (DoS/DDoS):\nIncludes L3/L4 volumetric attacks and L7 application floods. AWS reports a 29% year-over-year increase in application-layer events.\nApplication Vulnerabilities:\nCVEs and OWASP Top 10 risks such as XSS and injection attacks.\nBot Activity:\nScrapers, scalpers, spoofed clients, and rapidly growing AI-driven bots (GPT, Claude, Perplexity, ByteDance, Meta AI), with a 155% increase in traffic.\nRising Attack Trends DDoS incidents continue to escalate, with infrastructure-level and application-level impact patterns illustrated from 2021 to Q1 2025. AI-powered bots introduce new challenges across a diverse range of customer workloads. Defensive Strategies Using AWS Services Route 53 – DNS Resilience Global traffic management, auto-scaling, built-in DDoS protections, and a 100% uptime SLA. Amazon CloudFront – Edge Protection Routes traffic through the nearest Point of Presence. Restricts requests to HTTP/S ports, adds inline L3/L4 DDoS mitigation, and increases resilience through caching. With VPC Origin Protection, the ALB stays private and only CloudFront may access it. AWS Shield – Infrastructure Defense Edge \u0026amp; Border Protections: SYN proxies, packet inspection, geo controls, traffic scrubbing, and health-based mitigation. Advanced Features:\nAutomatically generated L7 rules, attack cost-protection credits, and support from the Shield Response Team (SRT). New CloudFront Flat-Rate Pricing Model Fixed Monthly Bundles Free ($0), Pro ($15), Business ($200), Premium ($1,000) No long-term contracts Included Services CloudFront CDN AWS WAF \u0026amp; DDoS protection Bot management \u0026amp; analytics Route 53 DNS CloudWatch Logs ingestion Serverless compute at the edge Monthly S3 storage credits Usage Allowances Free tier: 1M requests + 100 GB data Pro tier: 10M requests + 50 TB data Alerts triggered at 50%, 80%, and 100% consumption No overage fees — performance throttling may apply Important Exclusion WAF-blocked traffic and DDoS attack traffic do not count toward usage quotas → prevents unexpected bills during attacks, viral events, or sudden spikes. Purpose Eliminates unpredictable multi-service expenses. Prevents billing shocks caused by marketing surges, bot attacks, or unexpected virality. Encourages teams to focus on product development instead of infrastructure cost risks. AWS WAF – Core Functions and Deployment Web ACL Structure Rules, rule groups (managed, custom, marketplace), and default actions. Support for logging and traffic sampling. Rule Logic Inspects IPs, headers, bodies, and other attributes. Supports Allow, Block, Challenge, CAPTCHA, and Count actions. Highly customizable. Rate-Based Rules Track requests by IP or custom keys. Thresholds from 10 to 20M requests over 1–10 minutes. Labels for Fine-Grained Control Tag bot, fraud, geo, or match behaviors. Enable selective exceptions (e.g., allow XSS for specific endpoints if required). Recommended WAF Rule Ordering Allow/Block IPs, anti-DDoS managed rules, and rate-based protections. Anonymous/reputation checks, core attack protections, and specialized signatures. Bot/fraud rules and custom overrides, starting in Count mode to avoid false positives. Optimization Tips:\nPrioritize cheaper rules at the top. Use scope-down statements to reduce premium rule costs. Bot Control – Detailed Capabilities Known Bots Identified through User-Agent, IP ranges, or TLS fingerprints. Labeled by category and verification status. Evasive Bots Countered using JavaScript/CAPTCHA challenges, telemetry tracking, browser verification, and token-based anomaly detection. Indicators include missing tokens, abnormal request volumes, automation signatures, or inconsistent behavior. Mitigation ranges from Challenge to full Block. Shield Advanced – Additional Benefits Insights: CloudWatch metrics for traffic volume, drops, and timelines. Response Assistance: Guidance on log analysis, rule creation, and architectural hardening. Auto-Mitigation: Rules applied in Count mode first, then enforced; automatically removed after events. CloudFront as a Reverse Proxy – Technical Advantages Local TLS termination at edge locations. Traffic routed through the AWS global backbone. Offloads backend infrastructure (ALB, EC2, EKS, S3, API Gateway). Blocks malicious traffic at the edge, reducing compute, database, and network load — even without caching. Ideal Users for Advanced Protections Startups \u0026amp; SaaS platforms Public-facing websites and APIs E-commerce applications Education and community platforms Environments susceptible to bots or DDoS attacks Key Takeaways Security Perspective Treat public internet exposure as a primary threat vector. Use a layered approach combining L3/L4 and L7 defenses. Begin with Count mode, observe traffic, then enforce blocks gradually. Technical Safeguards Structure WAF rules thoughtfully, use labels for precision, and mix managed with custom protections. Apply bot interrogation methods, tokens, and behavior tracking to differentiate malicious automation. Strengthen DDoS resilience with Route 53, CloudFront, and Shield. Utilize CloudFront for edge filtering and infrastructure offloading. Cost \u0026amp; Strategy Improvements Flat-rate bundles deliver predictable pricing and exclude attack traffic. Remove risk of cost spikes from surges or threats. Build protection incrementally, validate through alerts and metrics. Evaluate long-term value: lower downtime, stronger security posture, predictable budgeting. Applying to Work Assess internet exposures and deploy WAF for OWASP/CVE risk mitigation. Implement bot defenses (challenges/tokens) on sensitive endpoints. Enable Shield for critical assets and configure rate-limiting. Start using CloudFront with managed WAF rules and refine with labels/exceptions. Use CloudWatch and SRT guidance for visibility and response enhancements. Pilot flat-rate tiers (Free/Pro) and scale to Business/Premium for production workloads. Event Experience Attending “Defense from Public Threat: AWS WAF \u0026amp; Application Protection” provided deep insights into securing modern applications against rapidly evolving online threats, enriched by recent AWS feature updates.\nExpert Insights AWS specialists shared practical threat intelligence and demonstrated integrations using architectural diagrams and global edge network maps. Real-world examples of DDoS escalations and rising AI bot traffic helped clarify how to apply WAF and Shield effectively. Hands-On Defensive Exploration Explored WAF configurations, rate rules, and label-based exceptions to handle false positives. Learned bot interrogation techniques using fingerprints, telemetry, and browser verification. Compared synchronous vs. asynchronous mitigations and inline vs. health-based detection. Leveraging New Features Understood how CloudFront and Route 53 strengthen perimeter security. Reviewed Shield’s auto-generated rules and cost protection benefits. Learned cost optimization tactics such as scope-down rules and attack-based exclusions. Collaboration and Discussions Interactive discussions covered rule design, incident response, and bridging conceptual knowledge with operational practice. Emphasis on evolving from foundational protection to layered, advanced security architectures. Lessons Learned Automated defenses are essential to counter rising bot and DDoS activity. Start with monitoring phases before enforcing rules to minimize disruption. Flat-rate pricing fundamentally improves cost predictability while strengthening security. "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Master IAM access control, policies, and identity management. Build VPC networking foundations for private/public environments. Understand and apply network firewalls (SG + NACL). Deploy EC2 instances inside custom VPC. Establish hybrid network connectivity using VPN. Tasks: Day Task Start Completion Reference 6 IAM Access Control 09/15 09/15 https://cloudjourney.awsstudygroup.com/ 7 Learn and build Amazon VPC 09/16 09/16 https://cloudjourney.awsstudygroup.com/ 8 VPC Firewalls (SG + NACL) 09/17 09/17 https://cloudjourney.awsstudygroup.com/ 9 Deploy EC2 in custom VPC 09/18 09/18 https://cloudjourney.awsstudygroup.com/ 10 Site-to-Site VPN Setup 09/19 09/19 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Deep understanding of IAM access control, including:\nUsers, Groups, Roles, Policies Inline vs Managed Policies Permission Boundaries Least Privilege Model IAM Best Practices from\n➝ https://cloudjourney.awsstudygroup.com/ Hands-on policy creation, including:\nCustom IAM policy JSON Attaching/detaching policies Using IAM Access Analyzer to validate permission risks Built a complete Amazon VPC from scratch, including:\nCustom IPv4 CIDR (10.0.0.0/16) Public + Private Subnets Internet Gateway NAT Gateway Routing Tables Subnet Associations Configured VPC Firewalls:\nSecurity Groups Allowed HTTP/HTTPS/SSH Instance-level firewall rules Network ACLs Stateless rules Custom inbound/outbound rules Understood differences between SG \u0026amp; NACL Deployed EC2 inside custom VPC:\nLaunched EC2 in Public Subnet Attached SG + Key Pair Connected via SSH Installed NGINX for connectivity test Validated routing and NAT behavior Set up Site-to-Site VPN concepts, including:\nCustomer Gateway (CGW) Virtual Private Gateway (VGW) VPN Tunnels How encrypted traffic flows through IPSec Routing propagation concepts Used CLI for VPC operations, such as:\ndescribe-vpcs describe-security-groups describe-route-tables describe-subnets Gained solid networking foundation for future EC2, RDS, and ECS deployments.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/5-workshop/5.3-create-environmnet/","title":"Create environment","tags":[],"description":"","content":"Create Elastic Beanstalk Application (Default VPC) Log in to the AWS Console and open the Elastic Beanstalk service.\nProceed to create a new Application with the following settings:\nApplication name: set according to the project name Platform: Java (Corretto) – matching the JDK version used by the application Environment type: Web Server VPC: use the Default VPC (Elastic Beanstalk automatically detects and selects it) Environment tier: Single Instance (no Load Balancer) Using the Default VPC simplifies the deployment process,\neliminating the need to manually configure subnets, route tables, or security groups.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/4-eventparticipated/4.3-event-3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AI/ML/GenAI on AWS – Workshop for Intern Students” Event Objectives Provide a comprehensive overview of AI, Machine Learning (ML), and Generative AI (GenAI) on AWS for intern students. Introduce AWS services supporting the entire ML lifecycle: data preparation, model training, deployment, and monitoring. Guide techniques in Generative AI, Prompt Engineering, and RAG (Retrieval-Augmented Generation). Encourage hands-on practice, experience sharing, and networking between students and AWS experts. Help students apply knowledge to internship projects or personal research, enhancing technical skills. Speakers Danh Hoang Huu Nghi - AI Engineer | Renova Cloud Dinh Le Hoang Anh - Cloud Engineer | First Cloud AI Journey Lam Tuan Kiet - Sr DevOps Engineer | FPT Software Key Highlights 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking with other students. Overview of the workshop content, learning objectives, and hands-on guidelines. Ice-breaker activities to facilitate interaction and create a collaborative learning environment. Overview of the AI/ML landscape in Vietnam: career opportunities, real-world applications, and common challenges faced by interns. 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker: End-to-end ML platform for data preparation, model training, tuning, and deployment. Data preparation \u0026amp; labeling: Standardizing data, handling missing values, and labeling for supervised learning. Model training \u0026amp; tuning: Training models, hyperparameter optimization, and performance evaluation. Deployment \u0026amp; MLOps: Deploying models to AWS, monitoring performance, and automating model updates. Live Demo: Walkthrough SageMaker Studio – upload datasets, train a model, deploy, and test. 10:30 – 10:45 AM | Coffee Break Networking, discussion, and casual interaction with mentors and other participants. 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Comparison of Claude, Llama, and Titan regarding language generation, knowledge integration, speed, and cost. Prompt Engineering: Techniques for creating effective prompts, Chain-of-Thought reasoning, and Few-shot learning. Retrieval-Augmented Generation (RAG): Integrating GenAI with knowledge bases to generate responses based on specific data. Bedrock Agents: Designing multi-step automated workflows combining multiple tools for complex tasks. Guardrails: Implementing safety measures to control content and prevent inappropriate outputs. Live Demo: Building a Generative AI chatbot using Bedrock, demonstrating prompt creation, RAG integration, and guardrails in practice. Key Takeaways AI/ML \u0026amp; GenAI Knowledge AWS provides a comprehensive solution for the entire ML lifecycle from data preparation, training, deployment, to monitoring. SageMaker allows interns to experience end-to-end ML workflow from raw data to deployable models. Amazon Bedrock enables Generative AI with foundation models, multi-step agents, and RAG. Prompt Engineering is crucial for generating accurate and relevant output. Guardrails ensure safety and compliance, teaching students how to deploy AI/ML responsibly. Applying to Internship Projects Deploy basic or advanced ML models using SageMaker for internship projects. Build Generative AI chatbots or applications using Amazon Bedrock. Integrate RAG to allow AI to answer questions using internal datasets. Practice prompt engineering to improve AI output quality. Set up guardrails to maintain content safety and quality. Learning Experience Hands-on practice helps students understand ML/GenAI workflows, deployment, and operations. Live demos illustrate integration of foundation models, RAG, agents, and prompt engineering. Workshop encourages interaction, networking, and knowledge sharing with AWS experts and peers. General Lessons Understanding the ML/GenAI lifecycle from data to deployment is essential. Prompt engineering and workflow management are key skills for building effective Generative AI applications. Using AWS tools increases productivity, reduces deployment time, and ensures safe AI outputs. Applying to Work / Internship Implement ML models on SageMaker to practice end-to-end workflow. Experiment with building GenAI chatbots using Bedrock for group or individual projects. Design RAG integration for chatbots to answer questions based on specific knowledge. Write effective prompts, analyze outputs, and continuously improve results. Learn to implement guardrails for safe, controlled AI behavior. Event Experience Interns gained practical exposure to AI/ML/GenAI workflows from experts. Live demonstrations provided clear guidance on deploying, training, and monitoring models. The workshop combined theory and practice, enhancing technical skills and fostering networking opportunities. Event Photos Insert your event photos here\nThis event significantly enhanced intern students’ understanding and practical skills in AI/ML/GenAI on AWS, covering everything from foundational concepts to hands-on deployment, while providing networking opportunities with experts and peers.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - AWS Weekly Update: Amazon Q Developer, AWS Step Functions, and More (Sep 22, 2025) This blog summarizes the key highlights from the AWS weekly update by Donnie Prakoso. It covers new features and services such as tangent mode in Amazon Q CLI, Amazon Bedrock model updates, VPC Reachability Analyzer expansion, AWS Step Functions enhancements, Amazon Corretto 25, and SageMaker HyperPod autoscaling. The article also introduces upcoming AWS events, opportunities to become an AWS Cloud Club Captain, and provides tips for developers to improve productivity using Amazon Q Developer.\nTags: Week in Review\nAuthor: Donnie Prakoso – Principal Developer Advocate at AWS, with 17+ years of experience in tech industries from telecom to startups. Passionate about helping developers explore technologies from microservices to AI/ML.\nBlog 2 - Reducing Waste and Improving Efficiency with AWS This blog guides public sector organizations on strategies to minimize waste and optimize efficiency in cloud adoption. Authored by Henrik Balle, Bhanu Jasthi, and Maia Haile, it presents a three-level approach: enterprise-wide digital transformation, AWS environment tuning, and service-level cost optimization. The article highlights OneGov initiatives, legacy modernization, AI-powered productivity tools like Amazon Q Business/Developer, and practical tips for cost control with AWS services such as SageMaker, Aurora, CloudFront, EC2 Spot, and S3 Intelligent-Tiering.\nTags: Artificial Intelligence, AWS Public Sector, best practices, cloud migration, government, mainframe migration, modernization\nAuthors:\nHenrik Balle – Principal Solutions Architect at AWS for U.S. public sector Bhanu Jasthi – Senior Solutions Architect at AWS with 20+ years in cloud architecture Maia Haile – Solutions Architect at AWS supporting public sector customers Blog 3 - Building Resilient Public Cloud Services: Updating Your Strategy This blog emphasizes the importance of resilience for public sector cloud services. Authored by Jeff Kratz, it discusses how AWS infrastructure, application architecture, software design, and operational excellence contribute to highly available, resilient services. The blog highlights traditional resilience strategies, the shift to modern cloud-native approaches including microservices and monitoring, and introduces an eight-part video series (resilience playbook) guiding organizations on best practices for building resilient applications.\nTags: AWS Public Sector, best practices, resiliency\nAuthor: Jeff Kratz – Leader of AWS Worldwide Public Sector Industry, focusing on cloud solutions for government, education, public health, and non-profit organizations.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Explore advanced Amazon EC2 operations. Install and configure LAMP stack on EC2. Deploy Node.js application on EC2. Implement IAM governance for cost control. Learn how applications access AWS services securely. Tasks: Day Task Start End Ref 11 Explore EC2 advanced features 09/22 09/22 https://cloudjourney.awsstudygroup.com/ 12 Install LAMP Web Server 09/23 09/23 https://cloudjourney.awsstudygroup.com/ 13 Deploy Node.js Applications 09/24 09/24 https://cloudjourney.awsstudygroup.com/ 14 Implement Cost Governance using IAM 09/25 09/25 https://cloudjourney.awsstudygroup.com/ 15 Application Access to AWS Services 09/26 09/26 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Explored EC2 deeply, including:\nInstance families (General, Compute, Memory optimized) Instance lifecycle \u0026amp; states EBS volumes (gp3, io2, throughput settings) ENI, public/private IP, Elastic IP Security Groups \u0026amp; Network Integration AMIs \u0026amp; Snapshots User Data automation scripts Installed full LAMP stack (Linux + Apache + MySQL + PHP):\nInstalled Apache web server Configured Virtual Hosts Installed PHP \u0026amp; essential extensions Connected Apache + PHP Installed \u0026amp; configured MySQL Deployed test PHP website Secured MySQL root account Deployed Node.js application, including:\nInstalled Node.js + NPM Installed PM2 process manager Created systemd startup for PM2 Configured firewall + Security Groups Reverse proxy using NGINX Deployed and tested Node.js app Implemented IAM cost governance:\nRead-only billing roles Budget alerts Cost explorer permissions IAM policies to restrict expensive resources Learned secure application access to AWS:\nIAM Roles for EC2 Instance metadata v2 (IMDSv2) Accessing S3 programmatically Using AWS SDK with temporary credentials CLI operations practiced:\ndescribe-instances Starting/stopping EC2 Creating EBS snapshots Viewing CPU metrics via CloudWatch CLI Acquired ability to deploy full-stack apps using EC2, LAMP, Node.js.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/5-workshop/5.4upload-and-deploy/","title":"Upload and deploy","tags":[],"description":"","content":"Upload the .jar file and deploy the application In the newly created Environment screen → select Upload and Deploy.\nProceed with:\nSelecting the built .jar file\nSetting a Version label (e.g., v1-initial)\nConfirming the deployment\nElastic Beanstalk will automatically:\nUpload the file to S3 Launch a single EC2 instance Start the Spring Boot application in the selected Java environment "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" Event 1 – GenAI-powered App-DB Modernization Workshop Event Name: GenAI-powered App-DB Modernization Workshop\nDate: 2025-10-03\nLocation: 26th Floor, Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nSummary:\nThis workshop focused on how Generative AI accelerates application modernization across the software lifecycle. Sessions covered legacy system pain points, DDD, event storming, microservices design, Amazon Q Developer, and modernization strategies.\nKey contents included:\nChallenges of legacy systems (scalability, cost, coupling, slow releases). Transitioning to microservices: queues, caching, message handling. DDD four-step method + context mapping (7 patterns). Event-driven architecture: pub/sub, streaming, sync vs async. Compute evolution from EC2 → ECS → Fargate → Lambda. Amazon Q Developer automating the SDLC. Modernization frameworks (7Rs) and phased migration. Experience \u0026amp; Lessons Learned:\nUnderstood how DDD and EDA reduce coupling and improve scalability. Hands-on event storming with bounded contexts. Practical exposure to Amazon Q Developer for code generation, refactoring, and documentation. Learned modernization strategies and serverless adoption. Strengthened communication with experts during networking sessions. Event 2 – Defense From Public Threat: AWS WAF \u0026amp; Application Protection Event Name: AWS Security Workshop – WAF, Shield \u0026amp; Edge Protection\nDate: 2025-11-19 Location: Ho Chi Minh City\nRole: Attendee\nSummary:\nThis workshop provided an in-depth exploration of modern internet threats and AWS protection mechanisms such as AWS WAF, Shield, CloudFront edge security, and the new flat-rate pricing bundles.\nKey contents included:\nCommon risks in 3-tier architectures and business impacts. DDoS trends, AI-powered bot surges (155% YoY), OWASP risks, CVEs. Defense layers: Route 53, CloudFront, WAF, Shield Advanced. Detailed WAF structure, rule logic, rate-based rules, labels, and ordering. Bot Control: known vs evasive bots, JavaScript challenges, tokens. Flat-rate bundles: predictable cost model excluding attack traffic. Experience \u0026amp; Lessons Learned:\nLearned to structure WAF rules safely using Count → Block. Understood bot detection strategies and behavior-based filtering. Hands-on viewing of WAF logs, labels, exceptions, and rate rules. Learned how CloudFront + Shield improves both security and cost. Gained confidence in analyzing threat patterns and designing layered defenses. Event 3 – AI/ML/GenAI on AWS – Workshop for Intern Students Event Name: AI/ML/GenAI on AWS – Hands-on Workshop\nDate: 2025-11-15 Location: Ho Chi Minh City\nRole: Attendee\nSummary:\nThis event delivered a complete overview of Machine Learning and Generative AI on AWS, combining presentations, demos, and hands-on practice across SageMaker and Bedrock.\nKey contents included:\nAI/ML landscape in Vietnam + career paths. SageMaker workflows: prepare → train → tune → deploy → monitor. Bedrock: Claude, Llama, Titan model comparison. Prompt engineering, CoT, few-shot techniques. RAG, Bedrock Agents, guardrails, safety best practices. Live demos: model deployment and Generative AI chatbot building. Experience \u0026amp; Lessons Learned:\nGained hands-on experience training and deploying ML models. Successfully built a Bedrock-based chatbot with RAG integration. Improved prompt engineering and model evaluation skills. Understood real ML workflows, MLOps, and deployment best practices. Benefited from networking with AWS experts and student peers. Event 4 – Cloud Mastery Series #2: DevOps on AWS Event Name: Cloud Mastery Series #2 – DevOps on AWS\nDate: 2025-11-17\nLocation: Ho Chi Minh City\nRole: Attendee\nSummary:\nA comprehensive workshop covering DevOps fundamentals, IaC, CI/CD, containerization, and observability using AWS services including CloudFormation, CDK, ECS/EKS, and CloudWatch.\nKey contents included:\nLimitations of ClickOps and the importance of automation. IaC using CloudFormation \u0026amp; CDK (L1–L3 constructs, CDK commands). Docker fundamentals and container workflow. ECS, EKS, App Runner architectures and use cases. CI/CD pipeline: CodeCommit → CodeBuild → CodeDeploy → CodePipeline. Observability using CloudWatch metrics/logs and X-Ray tracing. Experience \u0026amp; Lessons Learned:\nUnderstood IaC advantages for stability and repeatability. Observed real CI/CD pipelines triggering automated deployments. Learned to evaluate ECS, EKS, and serverless architectures. Strengthened understanding of monitoring and incident response. Identified DevOps best practices aligned with DORA metrics. Event 5 – AWS Well-Architected: Security Pillar Workshop Event Name: AWS Well-Architected – Security Pillar Workshop\nDate: 2025-11-29 Location: Ho Chi Minh City\nRole: Attendee\nSummary:\nThis workshop focused on the AWS Security Pillar, best practices for IAM, detection capabilities, data protection, incident response, and practical hands-on simulations.\nKey contents included:\nFive pillars of the AWS Security Framework. IAM hardening techniques: least privilege, policy boundaries, SCPs. Detection mechanisms using CloudWatch, GuardDuty, CloudTrail, Security Hub. Data protection using KMS, encryption policies, rotation strategies. Incident response workflows and validation exercises. Experience \u0026amp; Lessons Learned:\nLearned to evaluate IAM policies for misconfigurations. Understood layered detection and audit strategies. Strengthened skills in encryption, key handling, and access control. Practiced real incident simulations using AWS monitoring tools. Reinforced the importance of continuous security assessments. "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/4-eventparticipated/4.4-event-4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: “Cloud Mastery Series #2: DevOps on AWS” Event Objectives Strengthen understanding of DevOps culture and key performance indicators such as deployment speed and MTTR. Introduce AWS services that support automated CI/CD pipelines from code commit to production release. Explain Infrastructure as Code (IaC) using CloudFormation and CDK to standardize cloud resource provisioning. Provide fundamental knowledge of Docker and AWS container platforms for modern application deployment. Highlight monitoring, observability, and real-world DevOps best practices. Speakers Bao Huynh – AWS Community Builder Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Key Highlights Limitations of manual (“ClickOps”) operations Consumes significant developer/ops time, delaying feature delivery. Increases risk of misconfiguration due to human error. Creates difficulty maintaining identical environments across development stages. Understanding Infrastructure as Code (IaC) Manages infrastructure using code templates instead of console actions. Improves reliability, automation, and repeatability of deployments. AWS CloudFormation Overview AWS-native IaC service based on JSON/YAML. Supports lifecycle management for entire stacks. Offers drift detection to ensure the infrastructure remains aligned with templates. AWS Cloud Development Kit (CDK) Enables IaC using programming languages like TypeScript, Python, and Java. Construct hierarchy includes: L1: Raw CloudFormation mappings L2: High-level abstractions with smart defaults L3: Ready-to-use architectural patterns Common CDK commands: init, synth, deploy, diff, doctor Selecting IaC Tools Depends on cloud approach, team expertise, and existing architecture. CDK suits developer-centric teams; CloudFormation fits ops teams; Terraform fits hybrid/multi-cloud setups. Docker \u0026amp; Container Essentials Containers package applications and dependencies into portable, lightweight units. Ensures consistent behavior across all environments. Amazon ECR provides image hosting, vulnerability scanning, and version control. AWS Container Services ECS: Simple, AWS-native container orchestration. EKS: Full Kubernetes for complex and scalable workloads. App Runner: Serverless platform for deploying web applications quickly. CI/CD Pipeline on AWS CodeCommit → CodeBuild → CodeDeploy → CodePipeline form a complete delivery workflow. Supports zero-downtime deployments using blue/green, rolling, or canary strategies. Demonstrations showed automatic deployment triggered by code changes. Monitoring \u0026amp; Observability CloudWatch: Central place for logs, metrics, alerts, and dashboards. X-Ray: Visual tracing for performance bottlenecks and request flow analysis. Practical guidance for setting up alerting and improving incident response. Key Takeaways DevOps Mindset Encourages strong collaboration between development and operations. Uses DORA metrics to measure and improve team performance. Emphasizes continuous enhancement driven by business priorities. Technical Best Practices IaC ensures consistency, faster provisioning, and fewer configuration issues. CDK constructs help build scalable architectures quickly. Evaluate ECS/EKS based on complexity and operational needs. Reliable CI/CD pipelines reduce risk and accelerate feature delivery. Observability \u0026amp; System Reliability CloudWatch + X-Ray improve troubleshooting and incident detection. Gradual release models (canary/blue-green) help minimize downtime. Drift detection preserves environment correctness and stability. Modernization Strategy Assess existing workloads before planning modernization steps. Adopt incremental migration tactics for safer transitions. Consider modern services like App Runner for rapid deployment improvements. Applying to Work Implement Git workflows integrated with automated CodePipeline stages. Start experimenting with CDK to streamline cloud provisioning. Build dashboards in CloudWatch to track performance and reduce MTTR. Encourage internal discussions on container-first architectures. Prepare for AWS DevOps certifications to strengthen personal career growth. Event Experience Attending “Cloud Mastery Series #2: DevOps on AWS” delivered a practical and holistic view of DevOps implementation on AWS.\nLearning from Experts Speakers shared hands-on techniques and real-world project experiences. Comparisons between ECS, EKS, and serverless made architectural decisions clearer. Practical Technology Exposure Observed how automated CI/CD pipelines accelerate delivery. Understood how IaC eliminates inconsistencies and streamlines deployment workflows. Practiced interpreting metrics and logs using CloudWatch and X-Ray. Leveraging AWS Technologies CDK simplifies cloud infrastructure creation through code. App Runner demonstrated extremely fast deployment without server management. Interaction \u0026amp; Networking Q\u0026amp;A segments clarified common DevOps challenges and tool choices. Group discussions emphasized aligning DevOps transformation with business value. Lessons Learned Manual ClickOps introduces operational risk—automation is essential. Integration of IaC, CI/CD, and containers builds scalable, resilient systems. Strong observability improves reliability and reduces recovery time. Event Photos Insert your photos here\nThis event significantly enhanced my understanding of DevOps on AWS and provided actionable insights to apply these practices effectively in real-world projects.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand Amazon S3 fundamentals and storage concepts. Host a static website using S3 Website Hosting. Accelerate content delivery using CloudFront CDN. Implement S3 versioning and lifecycle management. Enable Cross-Region Replication (CRR) for durability. Tasks: Day Task Date Completed Ref 16 Starting with Amazon S3 09/29 09/29 https://cloudjourney.awsstudygroup.com/ 17 Enable Static Website on S3 09/30 09/30 https://cloudjourney.awsstudygroup.com/ 18 Accelerate Static Website with CloudFront 10/01 10/01 https://cloudjourney.awsstudygroup.com/ 19 Enable Bucket Versioning \u0026amp; Lifecycle Rules 10/02 10/02 https://cloudjourney.awsstudygroup.com/ 20 Cross-Region Replication Setup 10/03 10/03 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Mastered Amazon S3 storage fundamentals, including:\nBuckets, Objects, Folders, Prefixes Object immutability \u0026amp; durability model (11 9s) S3 Storage Classes (Standard, IA, Glacier, Deep Archive) Object-Level Security Concepts: Bucket Policy ACL Block Public Access controls Theory source: https://cloudjourney.awsstudygroup.com/ Created and configured S3 bucket, including:\nCustom bucket naming compliance (global namespace) Region selection for latency/cost Enabling/Disabling public access settings Understanding static website endpoint format Hosted a static website on S3, including:\nEnabled Static Website Hosting Set index.html \u0026amp; error.html Uploaded site objects (HTML, CSS, JS) Configured bucket policy for public read Verified website availability via S3 endpoint Integrated CloudFront with the S3 website, including:\nCreated new CloudFront distribution Configured S3 Origin Restricted bucket access using OAI (Origin Access Identity) Set caching behaviors (TTL, invalidation) Observed global CDN propagation Implemented S3 Versioning, enabling:\nObject history tracking Protection against accidental deletion Multiple version retention Configured S3 Lifecycle Rules, such as:\nTransitioning objects to IA / Glacier Expiration rules for unused data Clean-up for outdated versions Enabled Cross-Region Replication (CRR):\nCreated secondary bucket in another region Enabled versioning on both buckets Configured IAM role for replication Validated automatic replication triggering Conducted S3 CLI operations, such as:\naws s3 ls aws s3 sync aws s3 cp Viewing bucket policies \u0026amp; versioning config Gained the ability to deploy, secure, accelerate, and automate storage operations with S3.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/5-workshop/5.5-policy/","title":"Access the application and test","tags":[],"description":"","content":"Access the application and test the API using Postman After the deployment succeeds, Elastic Beanstalk provides a URL in the format:\nhttp://\u0026lt;environment-id\u0026gt;.\u0026lt;region\u0026gt;.elasticbeanstalk.com Use this URL to send requests from Postman to the Spring Boot API.\nExample:\nGET http://\u0026lt;env\u0026gt;.elasticbeanstalk.com/api/hello If the API responds with the expected data, the deployment is considered successful.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploying a Spring Boot REST API application to AWS Elastic Beanstalk Overview This workshop demonstrates the process of deploying a Spring Boot REST API application to Elastic Beanstalk in Single Instance mode.\nThis deployment model does not use a Load Balancer, does not configure Auto Scaling, and runs entirely within the default VPC of the AWS account.\nThe goals of this workshop include: Building the .jar file of the Spring Boot project. Creating a simple Elastic Beanstalk Application and Environment. Uploading and deploying the .jar file to the environment. Testing the API using Postman. Observing and analyzing logs through request logs. Content Workshop overview Prerequiste Create environment Upload and Deploy Access the application and test Monitoring and Clean up "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/4-eventparticipated/4.5-event-5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: “AWS Well-Architected – Security Pillar Workshop” Event Objectives Understand the five pillars of the AWS Security Framework Learn best practices for IAM, detection, data protection, and incident response Explore prevention strategies for cloud threats commonly seen Practice real-world simulations including IAM policy validation and detection workflows Security Foundation Importance of the Security Pillar in AWS Well-Architected Core principles: Least Privilege – Zero Trust – Defense in Depth Deep dive into the Shared Responsibility Model Review of common cloud threats in Vietnam (data exposure, weak IAM, misconfigurations) Speakers Tran Duc Anh Nguyen Tuan Thinh Nguyen Do Thanh Dat Pillar 1 — Identity \u0026amp; Access Management (IAM) Modern IAM Architecture (08:50 – 09:30) Avoid long-term credentials → adopt IAM Roles, federation, and short-lived tokens Use IAM Identity Center for SSO and centralized permission sets Guard multi-account security with SCP and permission boundaries Strengthen authentication with MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM policy \u0026amp; simulate access evaluation Key learning:\n→ IAM is the first line of defense, and misconfiguration is the #1 root cause of cloud breaches.\nPillar 2 — Detection \u0026amp; Monitoring Continuous Monitoring (09:30 – 09:55) AWS-native detection tools: CloudTrail, GuardDuty, Security Hub Enable logging across all layers: VPC Flow Logs, ALB Logs, S3 Access Logs Event-driven alerting pipelines using EventBridge “Detection-as-Code”: maintain detection rules with IaC + version control Key learning:\n→ Security works only if you monitor everything continuously and consistently.\nPillar 3 — Infrastructure Protection Network \u0026amp; Workload Security (10:10 – 10:40) VPC segmentation: private vs public subnet placement Security Groups vs NACLs: when to use each model Advanced protection: AWS WAF, Shield, Network Firewall Workload coverage: EC2 hardening, EKS/ECS baseline security Key learning:\n→ Infrastructure must adopt multi-layer protection, not just SG/NACL.\nPillar 4 — Data Protection Encryption, Keys \u0026amp; Secrets (10:40 – 11:10) AWS KMS: key policies, grants, automatic rotation Encryption at rest and in transit: S3, EBS, RDS, DynamoDB Secrets management with Secrets Manager \u0026amp; Parameter Store Data classification and guardrails for controlled access Key learning:\n→ Encryption is not optional; effective key management is equally important.\nPillar 5 — Incident Response IR Playbook \u0026amp; Automation (11:10 – 11:40) Incident lifecycle following AWS best practices Hands-on playbooks: Compromised IAM access key S3 public exposure EC2 malware detection Isolation workflow: snapshots, quarantining, evidence collection Automated response using Lambda and Step Functions Key learning:\n→ IR must be prepared before incidents, not built during the crisis.\nKey Takeaways Security Mindset Prevention \u0026gt; Remediation — eliminate long-lived credentials, enforce Zero Trust Avoid internet-facing resources except when absolutely necessary Everything should be defined and enforced using Infrastructure as Code Technical Knowledge Understand how detection sources (CloudTrail, DNS logs, VPC Flow Logs) form a complete picture Learn structured IR workflow instead of reacting ad-hoc Network Firewall + DNS Firewall help block malicious traffic proactively Managed threat signatures + GuardDuty = continuous threat intelligence Practical Skills Validating IAM policies with Access Analyzer Setting up org-level CloudTrail with EventBridge Using S3 Block Public Access effectively Applying encryption policies and secret rotation patterns Applying to Work Implement short-lived credentials and enforce MFA across all accounts Introduce GuardDuty, Security Hub, and centralized logging Review VPC architecture to limit public exposure Apply encryption standards consistently (S3, RDS, EBS, DynamoDB) Build simple IR runbooks for common incidents in the environment Automate alert → containment → remediation using Lambda Event Experience Attending the AWS Security Pillar Workshop helped reinforce both theoretical and practical aspects of cloud security.\nInsights from speakers The speakers highlighted how security failures often stem from misconfigurations, not from AWS itself. Their real-world examples clarified the importance of least privilege, network segmentation, and continuous monitoring. Practical sessions Running IAM access simulations gave me a clearer understanding of effective permission boundaries. The walkthrough of S3 exposure scenarios helped me understand how quickly data can leak without proper guardrails. Learning about AWS Network Firewall and DNS Firewall opened a new view on proactive threat blocking. Tools \u0026amp; automation GuardDuty findings, Active Threat Defense, and automated remediation workflows showed how security can scale across multi-account environments. I saw how IaC and “Detection-as-Code” can make security predictable and reproducible. Discussion \u0026amp; networking Discussions with AWS experts and participants helped me understand common security mistakes among Vietnamese companies. Conversations reinforced that security is a continuous process, not a one-time effort. Lessons learned Zero Trust + Least Privilege are non-negotiable. Automate everything: logging, detection, incident response. Prevention reduces 80% of potential incidents. Build IR playbooks before emergencies. Some event photos Add your event photos here\nThe workshop reshaped the way I think about cloud security—from reactive protection to proactive, automated, and continuously improving defense.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand relational databases on AWS using Amazon RDS. Deploy a managed database instance on AWS Lightsail. Learn container-based deployment with Lightsail Containers. Explore NoSQL concepts through DynamoDB. Begin interacting with AWS programmatically via SDK. Tasks: Day Task Date Completed Ref 21 Learn Amazon RDS 10/06 10/06 https://cloudjourney.awsstudygroup.com/ 22 Deploy a Database on Lightsail 10/07 10/07 https://cloudjourney.awsstudygroup.com/ 23 Deploy a Containers on Lightsail 10/08 10/08 https://cloudjourney.awsstudygroup.com/ 24 Work with Amazon DynamoDB 10/09 10/09 https://cloudjourney.awsstudygroup.com/ 25 Begin with AWS SDK 10/10 10/10 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Learned foundational concepts of Amazon RDS, including:\nDatabase Engines (MySQL, PostgreSQL, MariaDB) Reserved instances, Multi-AZ, Read Replicas Automated backups, snapshots, maintenance windows Parameter groups \u0026amp; security groups Private vs Public RDS endpoints Created an Amazon RDS instance, including:\nSelected DB engine version Chose instance class \u0026amp; allocated storage Configured security: Public accessibility SG allowing DB port 3306 Created initial DB \u0026amp; tested connection via MySQL Workbench Deployed a Lightsail Managed Database, including:\nCompared with RDS features Set up static private IP Interconnected app ↔ Lightsail DB Configured automatic backups + failover options Deployed an Application Using Lightsail Containers:\nBuilt container image Pushed to Lightsail Container Registry Created deployment + scale settings Configured environment variables Learned Amazon DynamoDB, including:\nPartition key, sort key Provisioned vs On-Demand capacity DynamoDB Streams Global \u0026amp; Local Secondary Indexes Best practices for NoSQL data modeling Performed DynamoDB operations, including:\nCreated table Inserted items Query \u0026amp; Scan operations via Console + CLI Started working with AWS SDK, including:\nInstalled AWS SDK for JavaScript / Python Wrote scripts to: Upload files to S3 Query DynamoDB List EC2 instances Used temporary security credentials via IAM Roles Gained practical experience in relational, NoSQL, and containerized application hosting.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/5-workshop/5.6-cleanup/","title":"Monitoring and Clean up","tags":[],"description":"","content":"Monitoring deployment logs and application logs Elastic Beanstalk provides two methods for viewing logs:\n1. Request Logs in Elastic Beanstalk Accessible via Logs → Request logs → Last 100 lines.\nThis log shows:\nThe Spring Boot startup process Error logs if the application fails Requests/responses processed by the runtime environment 2, CloudWatch Logs Elastic Beanstalk automatically pushes certain logs to CloudWatch,\nespecially system logs from the EC2 instance.\nClean up resources after deployment To avoid unexpected costs, it is recommended to delete the resources created after completing the workshop.\n1. Delete the Environment Go to Elastic Beanstalk → select the environment → Actions → Terminate environment.\nThis will remove the EC2 instance, the auto-generated security groups, and related resources.\n2. Delete the Application (if no longer needed) Elastic Beanstalk → Application → Actions → Delete Application.\n3. Delete files in S3 Elastic Beanstalk stores .jar versions inside an S3 bucket.\nGo to S3 → locate the bucket created by EB → delete the unnecessary versions.\n4. Review CloudWatch Logs You may delete the log groups if you want to clean up further.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my work and studies, I always strive to learn and perform my tasks to the best of my ability. However, unexpected events sometimes cause delays and affect my motivation. During my time at Amazon Web Services, I gained valuable experience, expanded my knowledge, and had the opportunity to meet enthusiastic and talented people who supported me in improving myself.\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ☐ ✅ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ✅ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ☐ ✅ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ☐ ✅ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Enhance self-study ability, maintain discipline in learning and researching. Organize working time more effectively and reduce delayed deadlines. Improve problem-solving skills and develop new technical and professional abilities. Strengthen communication skills, both written and verbal, in English and other foreign languages. Increase confidence in presenting ideas and reporting results. Be more proactive in seeking feedback and applying it to improve work performance. Build resilience to handle unexpected challenges and maintain motivation. Prioritize tasks better to balance between learning, work, and personal growth. "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Research and evaluate AWS services for final project architecture. Design complete system architecture diagram with AWS services. Build detailed database schema for application. Document architecture decisions and deployment strategy. Track learning journey and workshop participation. Tasks: Day Task Date Completed Ref 26 Research project services 10/13 10/13 https://cloudjourney.awsstudygroup.com/ 27 Draw architecture diagram 10/14 10/14 https://cloudjourney.awsstudygroup.com/ 28 Design database schema 10/15 10/15 https://cloudjourney.awsstudygroup.com/ 29 Fix diagram and write documentation 10/16 10/16 https://cloudjourney.awsstudygroup.com/ 30 Write Events Participated 10/17 10/17 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Completed detailed research on AWS services, including:\nCompute options: EC2, Lambda, Lightsail, Fargate Storage: S3, EFS, RDS, DynamoDB Networking: VPC, CloudFront, API Gateway, Route 53 Monitoring \u0026amp; DevOps tools Compared cost, complexity, and performance of each option Designed full AWS architecture diagram:\nVPC design with public/private subnets Internet Gateway + NAT Gateway routing EC2 or Lambda compute flows RDS/DynamoDB data layer CloudFront as CDN S3 for static content or file storage IAM roles for permissions CloudWatch for monitoring Used AWS Icons (official architecture set) Created database design, including:\nEntity–Relationship Diagram (ERD) Table structures Primary \u0026amp; foreign keys Normalization vs Optimization decisions Indexing strategy Schema migration plan Wrote technical documentation to support architecture, such as:\nHigh-level overview Component description Data flow diagrams API design Deployment plan IAM role assignments Monitoring \u0026amp; logging plan Refined diagrams based on design feedback, improving:\nSecurity posture (restrict SG rules) Cost (replace NAT Gateway with NAT Instance when appropriate) Performance (move static content to CloudFront) Finished Events Participated section, documenting:\nAll FCJ sessions attended Lessons learned Community participation activities Achieved ability to plan, design, and document production-ready AWS architectures.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/7-feedback/","title":"Feedback &amp; Reflection","tags":[],"description":"","content":"Sharing \u0026amp; Feedback 1. Overall Evaluation 1.1. Working Environment The working environment at AWS is very professional, friendly, and open. Everyone is always ready to support me when I encounter difficulties, even outside working hours. The workspace is modern, with clear processes that make it easy to focus and integrate.\nHowever, I think AWS could organize more internal activities or team bonding events to increase connection between interns and full-time staff.\n1.2. Support from Mentor / Team Admin The mentor guides in detail, explains clearly, and always encourages me to research myself rather than providing direct answers.\nThe team admin fully supports documents, training accounts, schedules, and related procedures, allowing me to focus on learning and practicing.\n1.3. Job Alignment with Major The tasks are directly related to cloud computing, security, networking, and DevOps — all align with my major.\nInterning at AWS helps me apply the knowledge learned at school to a real environment, especially when doing labs on system architecture and security.\n1.4. Learning Opportunities \u0026amp; Skill Development During the internship, I gained many valuable skills such as:\nUsing AWS services (EC2, S3, CloudFront, IAM, WAF, Shield, Lambda…) Thinking in building sustainable architecture \u0026amp; cost optimization Teamwork, communication, and problem-solving skills Report writing, information synthesis, and professional presentation skills Security, DevOps, and automation mindset The mentor shared a lot of practical experience, giving me a clearer career direction.\n1.5. Culture \u0026amp; Team Spirit AWS has a very positive work culture: respectful, transparent, proactive, continuously learning.\nWhen tasks are urgent, everyone supports each other wholeheartedly, regardless of role or seniority. This makes me feel like a real part of the team, even as an intern.\n1.6. Policies / Benefits for Interns AWS provides learning accounts, training materials, flexible schedules, and many internal workshops.\nThese are practical benefits that help interns learn quickly, grasp knowledge thoroughly, and have opportunities to develop themselves.\n2. Other Questions What are you most satisfied with during the internship? I am most satisfied with being directly exposed to a real cloud environment, receiving close mentor support, and doing practical labs. The learning environment is clear, professional, and very inspiring.\nIf recommending to friends, would you advise them to intern here? Definitely yes.\nBecause AWS is an ideal environment to learn cloud from basic to advanced levels, suitable for those aiming to be cloud engineers, security, DevOps, or backend developers.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn AWS CloudWatch monitoring system. Configure CloudWatch Logs, Metrics, and Dashboards. Set up CloudWatch Alarms with SNS notifications. Understand SQS queue models and message processing. Integrate application logging and monitoring workflows. Tasks: Day Task Date Completed Ref 31 Learn CloudWatch Monitoring 10/20 10/20 https://cloudjourney.awsstudygroup.com/ 32 Set up CloudWatch Logs \u0026amp; Metrics 10/21 10/21 https://cloudjourney.awsstudygroup.com/ 33 Configure SNS Notifications 10/22 10/22 https://cloudjourney.awsstudygroup.com/ 34 Learn SQS Queues 10/23 10/23 https://cloudjourney.awsstudygroup.com/ 35 Integrate Monitoring with Application 10/24 10/24 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Mastered AWS CloudWatch fundamentals, including:\nMetrics (system, AWS service metrics, custom metrics) Log Groups \u0026amp; Log Streams CloudWatch Agent installation EC2, Lambda, and API Gateway integrations Dashboard creation \u0026amp; visualization Knowledge source: https://cloudjourney.awsstudygroup.com/ Configured CloudWatch Logs, including:\nInstalled CloudWatch Agent on EC2 Configured amazon-cloudwatch-agent.json Pushed NGINX/Apache logs to Log Groups Verified log ingestion \u0026amp; timestamps Worked with CloudWatch Metrics, such as:\nCPUUtilization NetworkIn / NetworkOut DiskReadOps / DiskWriteOps Custom app metrics (via CLI) Created CloudWatch Alarms, including:\nCPU \u0026gt; 70% for 5 minutes → SNS notification StatusCheckFailed \u0026gt; 0 → Email alert Billing Alarm using USD threshold Configured SNS Topics for alerts:\nCreated Topic Subscribed email + verified endpoint Linked Alarm → SNS → Email pipeline Tested alarm triggers with scaled load Learned Amazon SQS message queueing system, including:\nQueue types: Standard vs FIFO Visibility Timeout Dead-Letter Queue (DLQ) Long polling vs short polling Use cases: microservices, decoupling applications Developed SQS workflows, including:\nCreated queues Sent/received messages via Console + CLI Batch message processing Purging queues Integrated application logs with CloudWatch, gaining:\nReal-time log viewing Metrics extraction from logs Monitoring alert flow Gained ability to monitor, alert, and decouple applications using CloudWatch + SNS + SQS.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Understand Elastic Beanstalk architecture and components. Deploy multi-tier applications using EB. Learn Load Balancers \u0026amp; environment tiers. Configure Auto Scaling groups and scaling triggers. Manage application lifecycle on EB. Tasks: Day Task Date Completed Ref 36 Introduction to Elastic Beanstalk 10/27 10/27 https://cloudjourney.awsstudygroup.com/ 37 Deploy App to EB 10/28 10/28 https://cloudjourney.awsstudygroup.com/ 38 Configure Load Balancer 10/29 10/29 https://cloudjourney.awsstudygroup.com/ 39 Configure Auto Scaling Policies 10/30 10/30 https://cloudjourney.awsstudygroup.com/ 40 Update \u0026amp; Redeploy Application 10/31 10/31 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Learned Elastic Beanstalk core concepts, including:\nEB Environments (Web Server vs Worker) EB Platform versions (Node.js, Python, PHP\u0026hellip;) EC2 Auto Scaling Groups \u0026amp; Load Balancers provisioned automatically Deployment strategies (Rolling, Immutable, Blue/Green) Deployed a Full Application on Elastic Beanstalk:\nCreated EB application \u0026amp; environment Uploaded initial version via ZIP EB automatically provisioned: EC2 instance Security Groups Auto Scaling Group Load Balancer Verified environment health + logs Configured Application Load Balancer (ALB):\nPath-based routing Health checks configuration Listener rules Sticky sessions off/on testing Implemented Auto Scaling Policies:\nTarget Tracking (CPU \u0026gt; 50%) Step Scaling (CPU \u0026gt; 70% add instance) Set min/max instance range Simulated load test to trigger scaling Performed EB lifecycle tasks:\nDeployed new application versions Performed environment updates \u0026amp; rollbacks Used .ebextensions to customize configuration Viewed logs from EB console (tail logs, bundle logs) CLI Experience:\nInstalled EB CLI eb init, eb create, eb deploy, eb status Version control of EB application builds Achieved ability to deploy and auto-scale production workloads using Elastic Beanstalk.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Understand serverless architecture and event-driven compute. Build compute functions with AWS Lambda. Deploy REST APIs using API Gateway (HTTP/REST). Integrate Lambda with DynamoDB, S3, SNS, EventBridge. Learn IAM roles \u0026amp; permissions for serverless systems. Tasks: Day Task Date Completed Ref 41 Learn AWS Lambda 11/03 11/03 https://cloudjourney.awsstudygroup.com/ 42 Build and Test Lambda Functions 11/04 11/04 https://cloudjourney.awsstudygroup.com/ 43 Create REST API with API Gateway 11/05 11/05 https://cloudjourney.awsstudygroup.com/ 44 Integrate Lambda with DB \u0026amp; S3 11/06 11/06 https://cloudjourney.awsstudygroup.com/ 45 IAM Roles for Serverless 11/07 11/07 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Mastered AWS Lambda foundations, such as:\nExecution model (ephemeral compute) Cold start vs warm start Memory \u0026amp; timeout settings Concurrency model Layers \u0026amp; extensions Built Lambda Functions:\nCreated handlers in Node.js/Python Logged events \u0026amp; context Used test events Returned API responses with correct JSON structure Integrated Lambda with AWS Services:\nDynamoDB (PutItem, GetItem) S3 (upload, read, delete) SNS (notifications) EventBridge (scheduled events / cron jobs) Created REST API using API Gateway:\nCreated API resources \u0026amp; methods Connected API → Lambda integration Configured CORS Set request/response mappings Deployed stages (dev, prod) Tested using Postman + curl Implemented IAM Roles for serverless:\nExecution role for Lambda Inline vs managed AWSLambdaBasicExecutionRole Principle of Least Privilege applied to Lambda IAM Created end-to-end serverless workflow, including:\nAPI Gateway → Lambda → DynamoDB S3 Upload → Lambda Trigger CRON schedule → Lambda automation CLI Experience:\naws lambda create-function aws lambda invoke aws apigateway get-rest-apis Gained production-level understanding of serverless systems and event-driven architecture.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Learn AWS DevOps tools for CI/CD automation. Create repositories using CodeCommit. Build and test applications using CodeBuild. Deploy applications using CodeDeploy. Automate entire CI/CD lifecycle using CodePipeline. Tasks: Day Task Date Completed Ref 46 Learn AWS DevOps Toolchain 11/10 11/10 https://cloudjourney.awsstudygroup.com/ 47 Set up CodeCommit Repo 11/11 11/11 https://cloudjourney.awsstudygroup.com/ 48 Build App with CodeBuild 11/12 11/12 https://cloudjourney.awsstudygroup.com/ 49 Deploy with CodeDeploy 11/13 11/13 https://cloudjourney.awsstudygroup.com/ 50 Create Full CI/CD Pipeline 11/14 11/14 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Learned AWS DevOps Ecosystem, understanding:\nCodeCommit (Git hosting) CodeBuild (build automation) CodeDeploy (deployment automation) CodePipeline (CI/CD orchestration) Artifact storage (S3) IAM Permissions for DevOps workflows Created CodeCommit Repository:\nConfigured Git credentials for IAM Cloned repo \u0026amp; pushed code Versioned application source via Git Configured CodeBuild, including:\nCreated build specifications (buildspec.yml) Installed dependencies Unit testing scripts Build artifacts output to S3 Parallel builds + build logs Deployed application using CodeDeploy, including:\nDeployment groups AppSpec configuration (appspec.yml) EC2 deployment lifecycle hooks Managed automatic rollback on failure Created Full CI/CD Pipeline, including:\nCodeCommit → CodeBuild → CodeDeploy → EC2 Configured event triggers on Git push Automated deployments on every commit Added manual approval stage for production Verified CI/CD end-to-end flow:\nMade Git commits Observed automatic build → deploy Checked logs for build/deploy stages Gained strong foundation in AWS DevOps automation for production readiness.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Learn Docker containerization fundamentals. Push images to Amazon ECR. Deploy services on Amazon ECS (EC2/Fargate). Configure ECS Networking and Auto Scaling. Understand container orchestration workflows. Tasks: Day Task Date Completed Ref 51 Learn Docker Basics 11/17 11/17 https://cloudjourney.awsstudygroup.com/ 52 Create ECR Repository 11/18 11/18 https://cloudjourney.awsstudygroup.com/ 53 Build \u0026amp; Push Images to ECR 11/19 11/19 https://cloudjourney.awsstudygroup.com/ 54 Deploy on ECS (EC2/Fargate) 11/20 11/20 https://cloudjourney.awsstudygroup.com/ 55 ECS Networking \u0026amp; Auto Scaling 11/21 11/21 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Learned Docker core concepts, including:\nImages vs Containers Dockerfile instructions (FROM, RUN, CMD, EXPOSE…) Multi-stage builds Tagging \u0026amp; versioning images Container networking Built Docker Images:\nCreated Dockerfile for Node.js app Optimized layers to reduce image size Used .dockerignore Tested containers locally Created Amazon ECR Repository, including:\nPrivate registry configuration Authenticated via AWS CLI Tagging \u0026amp; pushing images to ECR Deployed applications using ECS, including:\nCreated Task Definitions Configured CPU, memory, environment variables Created ECS Service Integrated ALB for service routing Set desired count for replicas Used AWS Fargate for serverless containers, achieving:\nNo need for EC2 hosts Fully managed compute Scaling based on CPU/memory thresholds Configured ECS Networking, including:\nVPC subnets selection Public/Private subnet deployment Security Group for tasks IAM Role for ECS Tasks Tested Auto Scaling with ECS, including:\nCPU-based scaling out/in Minimum \u0026amp; maximum task count Load test to trigger new tasks Gained ability to deploy containerized applications on AWS using ECS + ECR + Fargate.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Deploy final project based on the designed architecture. Integrate monitoring, CI/CD, serverless, and container components. Optimize performance, cost, and security across all services. Conduct full system testing and fix issues. Write complete final project documentation. Tasks this week: Day Task Date Completed 56 Deploy Full Final Project 11/24 11/24 57 Optimize Cost \u0026amp; Security 11/25 11/25 58 Performance \u0026amp; Load Testing 11/26 11/26 59 Fix Bugs \u0026amp; Improve Reliability 11/27 11/27 Week 12 Achievements: Successfully deployed complete project architecture, including:\nVPC (public/private subnets) EC2 or Lambda compute layer S3 + CloudFront static hosting RDS / DynamoDB data layer API Gateway endpoints CloudWatch monitoring dashboards CI/CD Pipeline for auto-deployment ECS containers where applicable Integrated all monitoring tools:\nCloudWatch Alarms (CPU, memory, Custom Metrics) Logging pipeline (CloudWatch Logs) SNS notifications for alerts Dashboard for system visibility Conducted cost optimization, including:\nRightsizing EC2 instances Applying S3 Lifecycle rules Moving infrequent objects to Glacier Configuring DynamoDB On-Demand when applicable Reviewing charges in Cost Explorer Performed performance testing, including:\nLoad tests for API endpoints Stress tests for EC2 Auto Scaling ECS task scale-out validation CloudFront latency checks globally Improved application reliability, including:\nFixed high-latency flows Resolved IAM permission gaps Fixed unhealthy target group issues Improved database indexing for faster queries Completed full project documentation, including:\nArchitecture diagram (final version) API documentation Deployment instructions Security guidelines Monitoring \u0026amp; alerting SOP Cost breakdown \u0026amp; optimization summary Achieved readiness for a real-world AWS production deployment.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Hong Duong\nPhone Number: 0336878898\nEmail: duongthse180648@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh City\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/09/2025 to 12/09/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/3-blogstranslated/3.1-blog1/","title":"3.1. Blog 1","tags":[],"description":"","content":"Bản tin hàng tuần của AWS: Amazon Q Developer, AWS Step Functions, Hạn chót AWS Cloud Club Captain, và hơn thế nữa (22 tháng 9, 2025) Bản tin hàng tuần của AWS: Amazon Q Developer, AWS Step Functions, Hạn chót AWS Cloud Club Captain, và hơn thế nữa (22 tháng 9, 2025) bởi Donnie Prakoso | 22 tháng 9 năm 2025 | Amazon Bedrock, Amazon Q Developer, Amazon VPC, AWS Step Functions, News | Permalink\nBa tuần trước, tôi đã đăng một bài viết về AWS Region mới tại New Zealand (ap-southeast-6). Điều này đã dẫn đến một cơ hội tuyệt vời để đến thăm New Zealand, nơi tôi gặp gỡ những nhà phát triển đầy nhiệt huyết và trình bày tại một số sự kiện bao gồm Serverless và Platform Engineering meetup, AWS Tools and Programming meetup, AWS Cloud Clubs tại Auckland, và AWS Community Day New Zealand.\nTrong quá trình tạo nội dung cho các bài thuyết trình này, tôi đã khám phá ra một tính năng hữu ích trong Amazon Q CLI được gọi là tangent mode. Tính năng này đã thay đổi cách tôi duy trì sự tập trung bằng cách tạo ra các điểm kiểm tra hội thoại, cho phép bạn khám phá các chủ đề phụ mà không làm mất mạch chính.\nTính năng này đang ở chế độ thử nghiệm, và bạn có thể kích hoạt nó bằng lệnh q settings chat.enableTangentMode true. Hãy thử và xem liệu nó có hữu ích cho bạn không.\nCác cập nhật tuần trước Dưới đây là một số cập nhật đã thu hút sự chú ý của tôi:\n● Các mô hình nền tảng mới trong Amazon Bedrock — Amazon Bedrock mở rộng lựa chọn mô hình với dòng mô hình Qwen, DeepSeek-V3.1, và dịch vụ hình ảnh Stability AI hiện đã chính thức ra mắt, mang đến cho các nhà phát triển quyền truy cập vào các mô hình đa ngôn ngữ mạnh mẽ và khả năng tạo hình ảnh tiên tiến cho các tác vụ như tạo văn bản, tạo mã, tạo hình ảnh, và giải quyết các vấn đề phức tạp.\n● Amazon VPC Reachability Analyzer mở rộng đến bảy vùng mới — Khả năng Network Access Analyzer hiện đã có sẵn ở các vùng bổ sung, giúp khách hàng phân tích và khắc phục sự cố kết nối mạng trong cơ sở hạ tầng VPC với phạm vi phủ sóng toàn cầu được cải thiện.\n● Amazon Q Developer hỗ trợ máy chủ MCP từ xa — Amazon Q Developer giờ đây tích hợp với các máy chủ Model Context Protocol (MCP) từ xa, cho phép các nhà phát triển mở rộng khả năng trợ lý AI của họ với các công cụ và nguồn dữ liệu tùy chỉnh để cải thiện quy trình phát triển.\n● AWS Step Functions nâng cấp Distributed Map với các tùy chọn nguồn dữ liệu mới — Step Functions giới thiệu các tùy chọn nguồn dữ liệu bổ sung và các tính năng quan sát cải tiến cho Distributed Map, giúp xử lý các khối lượng công việc song song quy mô lớn dễ dàng hơn với khả năng giám sát và gỡ lỗi tốt hơn.\n● Amazon Corretto 25 chính thức ra mắt — Phân phối OpenJDK 25 miễn phí, đa nền tảng của Amazon hiện đã chính thức ra mắt, cung cấp cho các nhà phát triển Java hỗ trợ dài hạn, cải tiến hiệu suất và cập nhật bảo mật để xây dựng các ứng dụng hiện đại.\n● Amazon SageMaker HyperPod giới thiệu Autoscaling — SageMaker HyperPod giờ đây hỗ trợ khả năng tự động mở rộng, cho phép các nhóm học máy điều chỉnh động tài nguyên tính toán dựa trên nhu cầu khối lượng công việc, tối ưu hóa cả hiệu suất và chi phí cho các công việc huấn luyện phân tán.\nCác cập nhật khác ● AWS được vinh danh là Leader trong Gartner Magic Quadrant 2025 cho AI Code Assistants – AWS đã được công nhận là Leader trong Gartner’s Magic Quadrant cho AI Code Assistants, làm nổi bật khả năng của Amazon Q Developer trong việc giúp các nhà phát triển viết mã nhanh hơn và an toàn hơn với các gợi ý được hỗ trợ bởi AI.\n● Trở thành AWS Cloud Club Captain – Chỉ còn vài ngày trước khi đóng đăng ký! Tham gia mạng lưới ngày càng phát triển của các sinh viên đam mê công nghệ đám mây bằng cách trở thành AWS Cloud Club Captain! Là một Captain, bạn sẽ tổ chức các sự kiện và xây dựng cộng đồng đám mây trong khi phát triển kỹ năng lãnh đạo. Cửa sổ đăng ký mở từ ngày 1 đến 28 tháng 9, 2025.\nCác sự kiện AWS sắp tới Hãy kiểm tra lịch của bạn và đăng ký cho các sự kiện AWS sắp tới cũng như AWS re:Invent và AWS Summits:\n● AWS AI Agent Global Hackathon – Đây là cơ hội để bạn khám phá sâu vào bộ công cụ AI sinh tạo mạnh mẽ của chúng tôi và tạo ra một thứ gì đó thực sự tuyệt vời. Từ ngày 8 tháng 9 đến ngày 20 tháng 10, bạn có cơ hội tạo ra các tác nhân AI bằng bộ dịch vụ AI của AWS, cạnh tranh để giành hơn 45.000 USD tiền thưởng và các cơ hội tiếp cận thị trường độc quyền.\n● AWS Gen AI Lofts – Bạn có thể tìm hiểu về các sản phẩm và dịch vụ AI của AWS với các phiên độc quyền, gặp gỡ các chuyên gia hàng đầu trong ngành, và có cơ hội kết nối giá trị với các nhà đầu tư và đồng nghiệp. Đăng ký tại thành phố gần bạn nhất: Mexico City (30 tháng 9 – 2 tháng 10), Paris (7–21 tháng 10), London (13–21 tháng 10), và Tel Aviv (11–19 tháng 11).\n● AWS Community Days – Tham gia các hội nghị do cộng đồng dẫn dắt với các buổi thảo luận kỹ thuật, hội thảo, và các phòng thí nghiệm thực hành do các chuyên gia AWS và các nhà lãnh đạo ngành từ khắp nơi trên thế giới dẫn dắt: Nam Phi (20 tháng 9), Bolivia (20 tháng 9), Bồ Đào Nha (27 tháng 9), và Manila (4–5 tháng 10).\nBạn có thể xem tất cả các sự kiện AWS sắp tới và các sự kiện khởi nghiệp AWS.\nĐó là tất cả cho tuần này. Hãy kiểm tra lại vào thứ Hai tới để đọc một Bản tin Hàng tuần khác!\nChúc bạn xây dựng vui vẻ!\n— Donnie\nTHẺ: Week in Review\nTác Giả Donnie Prakoso\nDonnie Prakoso là một kỹ sư phần mềm, tự xưng là barista, và là Principal Developer Advocate tại AWS. Với hơn 17 năm kinh nghiệm trong ngành công nghệ, từ viễn thông, ngân hàng đến các công ty khởi nghiệp. Hiện tại, anh tập trung vào việc giúp các nhà phát triển hiểu về nhiều công nghệ khác nhau để biến ý tưởng của họ thành hiện thực. Anh yêu thích cà phê và bất kỳ cuộc thảo luận nào về bất kỳ chủ đề nào từ microservices đến AI/ML.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/3-blogstranslated/3.2-blog2/","title":"3.2. Blog 2","tags":[],"description":"","content":"Xây dựng các dịch vụ đám mây công cộng kiên cường: Tại sao đã đến lúc cập nhật chiến lược của bạn AWS Public Sector Blog\nXây dựng các dịch vụ đám mây công cộng kiên cường: Tại sao đã đến lúc cập nhật chiến lược của bạn\nbởi Jeff Kratz | 22 tháng 9 năm 2025 | tại Best Practices, Public Sector, Resilience | Permalink\nKhách hàng trong khu vực công của chúng tôi dựa vào tính kiên cường (resilience) của Amazon Web Services (AWS) Cloud để có thể hoàn thành các sứ mệnh quan trọng của họ. Trước những rủi ro ngày càng tăng của các cuộc tấn công mạng (cyberattacks), các mối đe dọa môi trường như lũ lụt hoặc động đất, và các rủi ro vật lý như lỗi thiết bị (equipment failures), các nhà lãnh đạo khu vực công có một công việc ngày càng phức tạp. Tuy nhiên, người dân kỳ vọng các dịch vụ chính phủ như bầu cử hoặc gia hạn hộ chiếu phải hoạt động hiệu quả và đáng tin cậy như các nền tảng thương mại trực tuyến mà họ sử dụng.\nCác chính phủ và các ngành chịu sự quản lý như dịch vụ tài chính vận hành các dịch vụ số thiết yếu, chẳng hạn như ngân hàng hoặc các nền tảng thuế số (digital tax platforms). Những dịch vụ này là một phần cơ bản trong cuộc sống của chúng ta, và chúng cần phải tiếp tục hoạt động hoặc khôi phục sau các sự cố gián đoạn—một đặc tính được gọi là resilience (tính kiên cường). Ngoài ra, chúng phải duy trì hoạt động với thời gian ngừng hoạt động tối thiểu, trạng thái mà chúng tôi gọi là high availability (tính sẵn sàng cao).\nTrong lịch sử, tính kiên cường của các dịch vụ này chủ yếu phụ thuộc vào cơ sở hạ tầng vật lý. Trước khi có đám mây, để tăng tính kiên cường cho ứng dụng, bạn sẽ thêm một giá đỡ (rack) khác trong một trung tâm dữ liệu (data center) khác, và để cải thiện hơn nữa, bạn sẽ sử dụng một trung tâm dữ liệu cách xa hàng chục hoặc hàng trăm kilomet.\nNhưng giờ đây, tính kiên cường không chỉ đơn thuần là cơ sở hạ tầng. Cách các tổ chức xây dựng ứng dụng đang thay đổi không ngừng; cũng giống như bảo mật (security) là một trách nhiệm chung (shared responsibility), tính kiên cường cũng vậy. Điều quan trọng là các tổ chức cần tìm hiểu về các công cụ và kỹ thuật giúp họ xây dựng và quản lý các ứng dụng đám mây kiên cường và hiểu cách duy trì các dịch vụ kiên cường nhất cho cộng đồng mà họ phục vụ.\nXây dựng văn hóa vận hành kiên cường Để trả lời các câu hỏi này, AWS Principal Technologist Rob Charlton và đội ngũ Public Sector Industries đã tạo ra một loạt video chia sẻ lý do tại sao các cách tiếp cận truyền thống đối với tính kiên cường của dịch vụ số cần được cập nhật để tận dụng tối đa tính kiên cường mà đám mây mang lại.\nTrong loạt video gồm tám phần, được xem như một resilience playbook (sổ tay về tính kiên cường), Rob đưa người xem qua quá trình phát triển từ tính kiên cường tập trung vào cơ sở hạ tầng truyền thống đến cách tiếp cận toàn diện ngày nay, bao gồm microservices (dịch vụ vi mô), cơ sở hạ tầng, giám sát (monitoring), và sự xuất sắc trong vận hành (operational excellence).\nBuilding Resilient Cloud Services (Part 1): A modern approach | Amazon Web Services\nTrong tập một, Rob giới thiệu Resilience Equation (Phương trình Kiên cường), một khung công tác cho thấy cách cơ sở hạ tầng nền tảng của AWS kết hợp với kiến trúc ứng dụng (application architecture), thiết kế phần mềm (software design), và hoạt động (operations) để tạo ra các dịch vụ thực sự kiên cường. Thông qua các hình ảnh trực quan rõ ràng và các ví dụ thực tế, Rob khám phá cách các kiến trúc ứng dụng hiện đại đã thay đổi mô hình thất bại (failure patterns), cũng như mô hình trách nhiệm chung (shared responsibility model) về tính kiên cường giữa AWS và khách hàng.\nTiếp tục trong tập hai, Rob đi sâu vào cách mô hình lồng ghép toàn cầu độc đáo của cơ sở hạ tầng AWS—bao gồm các trung tâm dữ liệu (data centers), Availability Zones (Vùng sẵn sàng), và Regions (Khu vực)—là nền tảng cho các dịch vụ số kiên cường.\nTrong tập ba, Rob cùng với Senior Principal Security Solutions Architect Stephen “Squigg” Quigg đi sâu hơn vào cách AWS thiết kế các trung tâm dữ liệu với tư duy “chấp nhận thất bại” (embrace failure mindset)—sử dụng phần cứng được thiết kế riêng (purpose-built hardware), mã hóa lại phần mềm (re-coding software), và các biện pháp bảo mật chặt chẽ. Từ phân phối nguồn điện (power distribution) đến dự phòng mạng (networking redundancy), mọi khía cạnh được thiết kế để tối đa hóa tính kiên cường trong khi nâng cao hiệu suất.\nBuilding Resilient Cloud Services (Part 4): Why multi–Availability Zone applications?\nLoạt video tiếp tục với việc khám phá các dịch vụ cụ thể của AWS, với Rob hướng dẫn người xem qua các mô hình triển khai (deployment models) của các dịch vụ AWS cụ thể, nhấn mạnh cách các lựa chọn kiến trúc này ảnh hưởng đến tính kiên cường và tính sẵn sàng của các ứng dụng khu vực công.\nTrong tập bốn, Rob quay lại với cơ sở hạ tầng để giải thích cách chuyển ứng dụng tại chỗ (on-premises application) sang đám mây giúp giảm thiểu các rủi ro lớn nhất, trước khi chuyển sang các đội ngũ đứng sau các dịch vụ AWS, một số thực tiễn vận hành của họ, và cách họ triển khai các bản cập nhật trong tập năm. Loạt video kết thúc với các cuộc thảo luận về dịch vụ và giám sát (monitoring). Với các mẹo cụ thể xuyên suốt, loạt video kết thúc bằng một loạt các thực tiễn tốt nhất (best practices) để xây dựng các ứng dụng đám mây kiên cường trong khu vực công.\nBuilding Resilient Cloud Services 8: Building resilient applications on AWS\nKết luận Dù bạn đang hoạt động trong lĩnh vực dịch vụ tài chính, chính phủ liên bang, tiểu bang, địa phương, hay bất kỳ ngành nào yêu cầu các dịch vụ có tính sẵn sàng cao (highly available services), loạt video này sẽ cho bạn thấy cách cơ sở hạ tầng toàn cầu, thiết kế dịch vụ, và các thực tiễn vận hành của AWS có thể tạo ra nền tảng kiên cường cho các khối lượng công việc quan trọng nhất của bạn.\nXem toàn bộ loạt video tại đây, và đọc thêm về tính kiên cường của AWS Cloud tại đây.\nTHẺ: AWS Public Sector, best practices, resiliency\nTác Giả Jeff Kratz\nJeff dẫn dắt các hoạt động kinh doanh của AWS Worldwide Public Sector Industry và Nonprofit, phục vụ các tổ chức chính phủ, giáo dục, y tế công cộng, và phi lợi nhuận. Jeff định hướng việc tạo ra, hiện đại hóa, và thực hiện các giải pháp đám mây quan trọng đối với sứ mệnh (mission-critical cloud solutions) ảnh hưởng đến hàng triệu người trên toàn cầu.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/3-blogstranslated/3.3-blog3/","title":"3.3. Blog 3","tags":[],"description":"","content":"Hướng dẫn giảm thiểu lãng phí và nâng cao hiệu quả với AWS AWS Public Sector Blog\nHướng dẫn giảm thiểu lãng phí và nâng cao hiệu quả với AWS\nbởi Henrik Balle, Bhanu Jasthi, và Maia Haile | 22 tháng 9 năm 2025 | tại Artificial Intelligence, Best Practices, Cloud Adoption, Generative AI, Government, Mainframe Migration, Migration Solutions, Public Sector | Permalink\nGiới thiệu Tổng cục Quản lý Dịch vụ Hoa Kỳ (U.S. General Services Administration - GSA) gần đây đã công bố một thỏa thuận OneGov đột phá với Amazon Web Services (AWS) nhằm cung cấp khoản tiết kiệm lên đến 1 tỷ USD cho việc áp dụng công nghệ đám mây, hiện đại hóa và đào tạo cho các cơ quan liên bang. Cùng với thông báo này là các giải pháp để giảm lãng phí và nâng cao hiệu quả.\nĐể giúp các tổ chức đẩy nhanh các sáng kiến này và giảm thiểu gian lận, lãng phí và lạm dụng (FWA), hướng dẫn này đưa ra các chiến lược thực tiễn để tối ưu hóa chi tiêu công nghệ đồng thời cải thiện hiệu quả vận hành. Chúng tôi sẽ trình bày một cách tiếp cận ba cấp độ để tối ưu hóa:\nSáng kiến chuyển đổi số toàn cơ quan Tinh chỉnh môi trường AWS Cải tiến ở cấp độ dịch vụ Mỗi cách tiếp cận bao gồm các tài nguyên và các bước hành động mà các tổ chức có thể bắt đầu thực hiện ngay hôm nay.\nSáng kiến chuyển đổi số toàn cơ quan Chuyển đổi công nghệ thông tin quy mô lớn bằng cách sử dụng các ưu đãi OneGov bắt đầu với việc migration và modernization thông qua áp dụng công nghệ đám mây. Ngoài việc hưởng lợi từ thỏa thuận OneGov, nghiên cứu của The Hackett Group cho thấy các tổ chức chuyển đổi sang AWS đạt được mức giảm chi phí cơ sở hạ tầng trung bình 20%, với các tổ chức hàng đầu đạt được mức tiết kiệm lên đến 47%. Ngoài hiệu quả chi phí, các cơ quan còn đạt được mức tăng 66% về năng suất cơ sở hạ tầng và chuyển hướng 29% thời gian để đổi mới sáng tạo.\nDù bạn chọn thoát hoàn toàn khỏi các trung tâm dữ liệu (data centers) hay dần dần di chuyển các ứng dụng theo thời gian, bước đầu tiên quan trọng là thu thập dữ liệu danh mục ứng dụng (application portfolio data). Dữ liệu này phải được đánh giá dựa trên bảy chiến lược di chuyển phổ biến, được gọi là 7 Rs: refactor, replatform, repurchase, rehost, relocate, retain, và retire. Theo các chiến lược này, các cơ quan có thể xác định con đường di chuyển tốt nhất cho từng ứng dụng.\nĐối với một số khối lượng công việc được xây dựng trên các công nghệ được cấp phép thương mại, bạn cũng có thể cân nhắc chương trình AWS Optimization and Licensing Assessment (AWS OLA) bổ sung, giúp tăng hiệu quả cấp phép lên đến 60% bằng cách xác định các giấy phép sử dụng chưa đầy đủ và đề xuất các lựa chọn thay thế tiết kiệm chi phí hơn.\nCác hệ thống kế thừa (legacy systems) tiêu tốn một phần ngân sách công nghệ thông tin không cân đối. Với AWS Transform, các ứng dụng .NET có thể được hiện đại hóa nhanh hơn đến bốn lần đồng thời giảm chi phí vận hành lên đến 40% thông qua việc loại bỏ giấy phép Windows. Việc di chuyển VMware loại bỏ các chi phí cấp phép bên thứ ba đắt đỏ và tăng tốc độ chuyển đổi mạng lên đến 80 lần. Với sức mạnh của agentic AI, việc hiện đại hóa mainframe rút ngắn thời gian từ vài năm xuống vài tháng, giảm chi phí mainframe liên tục và bảo toàn logic kinh doanh quan trọng.\nCác cơ quan liên bang có thể đơn giản hóa hoạt động trung tâm liên lạc (contact center) và loại bỏ lãng phí bằng cách hợp nhất nhiều nền tảng trung tâm liên lạc thành một nền tảng dựa trên đám mây thống nhất. Amazon Connect cung cấp mô hình pay-as-you-go và không yêu cầu cam kết tối thiểu. Các khả năng AI tích hợp tạo ra một vòng lặp hiệu quả mạnh mẽ, cho phép tự phục vụ người gọi, hỗ trợ nhân viên theo thời gian thực và tự động tạo tóm tắt, đồng thời triển khai các AI agents cho các công việc thường xuyên.\nĐể tìm hiểu cách khách hàng đã giảm 60% khối lượng cuộc gọi và giảm 50% thời gian đào tạo nhân viên, hãy truy cập Unleash AI để chuyển đổi mọi tương tác với khách hàng.\nCác cơ quan ngày càng tìm kiếm cải tiến trong năng suất nhân viên. Amazon Q Business giúp generative AI dễ dàng tiếp cận một cách an toàn cho mọi người trong tổ chức. Amazon Q Developer có thể tăng năng suất của nhà phát triển lên đến 40%, giúp họ viết mã nhanh hơn và an toàn hơn.\nTinh chỉnh môi trường AWS Cách tốt nhất để tối ưu hóa chi tiêu lãng phí là tránh các chi phí không cần thiết ngay từ đầu. AWS khuyến nghị sử dụng các phương pháp bảo mật tốt nhất trong AWS Identity and Access Management (IAM) khi thiết lập người dùng mới trong môi trường AWS. Điều này bao gồm việc thường xuyên xem xét và xóa các đặc quyền không sử dụng.\nKhách hàng có thể tinh chỉnh thêm bằng cách sử dụng AWS Service Catalog, cung cấp cho người dùng cuối một cổng để khám phá và khởi chạy các sản phẩm tuân thủ chính sách tổ chức và giới hạn ngân sách.\nAWS cung cấp một bộ giải pháp để hỗ trợ quản lý và tối ưu hóa chi phí:\ntheo dõi chi phí và mức sử dụng consolidated billing lập ngân sách và dự báo tối ưu hóa tài nguyên và giá Các tài nguyên như backups phát sinh chi phí ngay khi tạo. AWS Backup loại bỏ chi phí quản lý phần mềm sao lưu và cơ sở hạ tầng, đồng thời cung cấp các tính năng tiết kiệm chi phí thông qua automated lifecycle policies.\nBạn có thể giảm chi phí bằng cách:\nchuyển bản sao lưu ít được truy cập sang cold storage tự động xóa bản sao lưu hết hạn SageMaker Unified Studio cho phép tìm và truy cập dữ liệu trong tổ chức để phân tích và xây dựng ứng dụng generative AI.\nNếu vẫn dùng công cụ hiện có, bạn có thể giảm chi phí dữ liệu bằng kiến trúc data mesh sử dụng AWS Lake Formation để quản trị dữ liệu phi tập trung.\nTối ưu hóa chi phí ở cấp độ dịch vụ Kiến trúc serverless giúp giảm total cost of ownership bằng mô hình pay-for-value. AWS Well-Architected Framework cung cấp best practices cho serverless.\nĐối với ứng dụng chạy máy ảo:\nAWS Graviton mang lại hiệu suất giá tốt hơn đến 40%. Auto Scaling duy trì hiệu suất ổn định với chi phí thấp. Chuyển từ cơ sở dữ liệu thương mại sang Amazon Aurora có thể giảm chi phí xuống còn 1/10.\nNâng cấp từ EBS gp2 → gp3 tiết kiệm đến 20%.\nAmazon CloudFront giảm độ trễ và giúp tiết kiệm chi phí phân phối nội dung.\nAWS Compute Optimizer giúp rightsizing EC2 và RDS.\nSavings Plans tiết kiệm đến 72% so với On-Demand.\nAmazon EC2 Spot Instances tiết kiệm đến 90% cho batch, analytics và AI pre-training.\nAmazon S3 Intelligent-Tiering tự động tối ưu chi phí lưu trữ.\nVới generative AI:\ndùng Amazon Q Business / Developer trước khi tự xây dựng dùng mô hình nhỏ như Amazon Nova Micro tiết kiệm đến 70% mô hình distilled trên Bedrock nhanh hơn 500% và rẻ hơn 75% dùng S3 Vectors giảm chi phí RAG lên đến 90% prompt caching giảm chi phí inference 90% Kết luận Các khuyến nghị trong tài liệu này sẽ giúp bạn giảm lãng phí và tối ưu hóa chi phí dù bạn đang ở đâu trên hành trình đám mây của mình. Để đi sâu hơn, hãy tham khảo Cost Optimization Pillar của AWS Well-Architected Framework.\nLiên hệ đội ngũ AWS của bạn để tìm hiểu cách tối đa hóa lợi ích từ thỏa thuận OneGov.\nTHẺ: Artificial Intelligence, AWS Public Sector, best practices, cloud migration, government, mainframe migration, modernization\nTác Giả Henrik Balle Principal solutions architect tại AWS, hỗ trợ khu vực công Hoa Kỳ. Anh làm việc với nhiều chủ đề từ machine learning đến security và governance quy mô lớn.\nBhanu Jasthi Senior solutions architect tại AWS, với hơn 20 năm kinh nghiệm trong cloud architecture, disaster recovery và high availability.\nMaia Haile Solutions architect tại AWS khu vực Washington DC, chuyên hỗ trợ khách hàng khu vực công với các giải pháp AWS well-architected.\n"},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://dwayne1420.github.io/fcj-workshop-template-main/tags/","title":"Tags","tags":[],"description":"","content":""}]